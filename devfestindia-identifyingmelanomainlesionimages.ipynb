{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1g8w3556Xxp_mKJZ7lH6b55AuNx_QIBr5)"},{"metadata":{},"cell_type":"markdown","source":"# **SIIM**\n\n\nThe Society for Imaging Informatics in Medicine (SIIM) is the leading healthcare professional organization for those interested in the current and future use of informatics in medical imaging. The society's mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1Yhk63WM5MCTj0KLhhdGeiXsp8oaSxhTK)\n\nImage Source : siim.org"},{"metadata":{},"cell_type":"markdown","source":"# **ISIC :**\n\nThe International Skin Imaging Collaboration: Melanoma Project is an academia and industry partnership designed to facilitate the application of digital skin imaging to help reduce melanoma mortality. When recognized and treated in its earliest stages, melanoma is readily curable. Digital images of skin lesions can be used to educate professionals and the public in melanoma recognition as well as directly aid in the diagnosis of melanoma through teledermatology, clinical decision support, and automated diagnosis.\n"},{"metadata":{},"cell_type":"markdown","source":"# **Skin Cancer :**\n\nSkin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detectionâ€”potentially aided by data scienceâ€”can make treatment more effective.\n"},{"metadata":{},"cell_type":"markdown","source":"# **Problem Statement :**\n\nIn this workshop , youâ€™ll identify melanoma in images of skin lesions and youâ€™ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=11uCu4Bl5YHEi_xcfZ1Q86W3sOhJ6tvLy)"},{"metadata":{},"cell_type":"markdown","source":"# **Installing the Libraries**"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:07.768661Z","iopub.status.busy":"2020-09-29T17:35:07.767806Z","iopub.status.idle":"2020-09-29T17:35:29.46515Z","shell.execute_reply":"2020-09-29T17:35:29.464182Z"},"papermill":{"duration":21.729797,"end_time":"2020-09-29T17:35:29.465301","exception":false,"start_time":"2020-09-29T17:35:07.735504","status":"completed"},"tags":[],"id":"bqeZJ9juDVzv","trusted":true},"cell_type":"code","source":"!pip install -q --upgrade pip\n!pip install -q efficientnet","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-09-29T17:35:29.509576Z","iopub.status.busy":"2020-09-29T17:35:29.508858Z","iopub.status.idle":"2020-09-29T17:35:36.648898Z","shell.execute_reply":"2020-09-29T17:35:36.648021Z"},"papermill":{"duration":7.165158,"end_time":"2020-09-29T17:35:36.649056","exception":false,"start_time":"2020-09-29T17:35:29.483898","status":"completed"},"tags":[],"id":"C5TEL9qDDV0F","trusted":true},"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom functools import partial\nimport efficientnet.tfkeras as efn\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TPU\n\n\n![](https://drive.google.com/uc?id=1q6AUi9XZRRWBjov49PSl3thB9idGsUKV)"},{"metadata":{},"cell_type":"markdown","source":"Tensor Processing Units (TPUs) are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads.It's easy to run replicated models on Cloud TPU using High-level Tensorflow APIs .\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1-EAdsgcuVsFKO1ru00TTrKePRnv-6FHv)"},{"metadata":{},"cell_type":"markdown","source":"# Performance : \n\nTPU can achieve a high computational throughput on massive multiplications and additions for neural networks , at blazingly fast speeds with much less power consumption and smaller footprint.\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=18f8gdJOsNad_ZBESxhxxo2SVIf-IaYGR)"},{"metadata":{},"cell_type":"markdown","source":"ðŸ“Œ **TPU Initialization:** TPUs  are usually on Cloud TPU workers and hence have to be connected to remote clusters and then initialized .\n\nðŸ“Œ **Distribution strategies :** A distribution strategy is an abstraction that can be used to drive models on CPU, GPUs or TPUs. "},{"metadata":{},"cell_type":"markdown","source":"Credit : The code has been adapted from @mgornergooglestater kernel on Flower classification on TPUs"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-09-29T17:35:36.785423Z","iopub.status.busy":"2020-09-29T17:35:36.732271Z","iopub.status.idle":"2020-09-29T17:35:41.909695Z","shell.execute_reply":"2020-09-29T17:35:41.91053Z"},"papermill":{"duration":5.242582,"end_time":"2020-09-29T17:35:41.910774","exception":false,"start_time":"2020-09-29T17:35:36.668192","status":"completed"},"tags":[],"id":"hyQk56psDV0U","outputId":"57f42752-745e-490e-aa3a-a30c605875bc","trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":3,"outputs":[{"output_type":"stream","text":"Device: grpc://10.0.0.2:8470\nNumber of replicas: 8\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Note:</b> TPUs read data directly from <code>Google Cloud Storage (GCS)</code> This Kaggle utility will copy the dataset to a GCS bucket co-located with the TPU. If you have multiple datasets attached to the notebook, you can pass the name of a specific dataset to the <code>get_gcs_path function</code>get_gcs_path function. The name of the dataset is the name of the directory it is mounted in. Use <code>!ls /kaggle/input/</code> to list attached datasets.\n</div>"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=15QPJMCidq3gQM8oCZ6NNlRQHnImV28wP)"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:41.968507Z","iopub.status.busy":"2020-09-29T17:35:41.967789Z","iopub.status.idle":"2020-09-29T17:35:42.291702Z","shell.execute_reply":"2020-09-29T17:35:42.290784Z"},"papermill":{"duration":0.360735,"end_time":"2020-09-29T17:35:42.29187","exception":false,"start_time":"2020-09-29T17:35:41.931135","status":"completed"},"tags":[],"id":"1TNm_-hkDV0o","trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [1024, 1024]","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF Records :\n\nðŸ“ŒTF-Record is Tensorflowâ€™s own binary storage format.\n\nðŸ“ŒAny byte-string that can be decoded in TensorFlow could be stored in a TFRecord file. Examples include: Lines of text, JSON ,encoded image data, or serialized tf.Tensors .\n\nðŸ“ŒA TFRecord file contains a sequence of records and can only be read sequentially.Each record contains a byte-string, for the data-payload, plus the data-length, and CRC32C (32-bit CRC using the Castagnoli polynomial) hashes for integrity checking.\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=10r4NCvxUdK6IhEFWI1UxwVUY4PYYMMVG)"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.5044Z","iopub.status.busy":"2020-09-29T17:35:42.503397Z","iopub.status.idle":"2020-09-29T17:35:42.507098Z","shell.execute_reply":"2020-09-29T17:35:42.507707Z"},"papermill":{"duration":0.196123,"end_time":"2020-09-29T17:35:42.507902","exception":false,"start_time":"2020-09-29T17:35:42.311779","status":"completed"},"tags":[],"id":"SUucXwzfDV05","outputId":"32c83bbf-f221-4408-dc25-98e4673d8a8e","trusted":true},"cell_type":"code","source":"TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    tf.io.gfile.glob(GCS_PATH + '/tfrecords/train*.tfrec'),\n    test_size=0.1, random_state=42\n)\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/tfrecords/test*.tfrec')\nprint('Train TFRecord Files:', len(TRAINING_FILENAMES))\nprint('Validation TFRecord Files:', len(VALID_FILENAMES))\nprint('Test TFRecord Files:', len(TEST_FILENAMES))","execution_count":5,"outputs":[{"output_type":"stream","text":"Train TFRecord Files: 14\nValidation TFRecord Files: 2\nTest TFRecord Files: 16\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1qomd1gdCbw7CA-vdlqtBWAYqsU6-aCtN)\n\nImage Source : mc.ai"},{"metadata":{},"cell_type":"markdown","source":"# TF-Data\n\n![](https://drive.google.com/uc?id=10M07nrMGrKKVP0KGwAqsm7oNK1mXexsD)"},{"metadata":{},"cell_type":"markdown","source":"ðŸ“ŒThe tf.data API enables you to build complex input pipelines from simple, reusable pieces. The tf.data API also makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations.\n\nðŸ“ŒThe tf.data API introduces a tf.data.Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n\nðŸŽ¯ **Input Pipeline :**\n\n tf.data.TFRecordDataset() is used to create an input pipeline for data stored in TFRecord format .\n \nðŸŽ¯ **Transformations :**\n\nThe Dataset object can be transformed into a new Dataset by chaining method calls on the tf.data.Dataset object . Some of the transformations which can be applied are Dataset.map() , Dataset.batch() , Dataset.shuffle() , Dataset.prefetch() .\n\nThe Dataset object is a Python iterable which  it possible to consume its elements using a for loop .\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1CLfEMEn9uiPGjhgVZc620djKmBQrwSZV)"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1x383ghyybTV0jQqBSlHDEc8FWD8AqaHz)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_to_tensor(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_to_tensor(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum","execution_count":7,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.664302Z","iopub.status.busy":"2020-09-29T17:35:42.663307Z","iopub.status.idle":"2020-09-29T17:35:42.666786Z","shell.execute_reply":"2020-09-29T17:35:42.66616Z"},"papermill":{"duration":0.032205,"end_time":"2020-09-29T17:35:42.666907","exception":false,"start_time":"2020-09-29T17:35:42.634702","status":"completed"},"tags":[],"id":"o3mSVhP0DV1o","trusted":true},"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation :\n\nðŸŽ¯  Data augmentation is a technique through which one can increase the size of the data for the training of the model without adding the new data. Techniques like padding, cropping, rotating, and flipping are the most common methods that are used over the images to increase the data size. \n\n<div class=\"alert alert-block alert-info\">\n<b>Note:</b> Both <code>Keras Preprocessing Layers</code> and <code>tf.image</code> can be used for data augmentation. For finer control , <code>tf.image</code> is preferred.\n</div>\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1_uGNygnCZ--G_hOP7zEr_oscGjvnBnob)\n\nImage Source : medium.com"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.714532Z","iopub.status.busy":"2020-09-29T17:35:42.713412Z","iopub.status.idle":"2020-09-29T17:35:42.718081Z","shell.execute_reply":"2020-09-29T17:35:42.717423Z"},"papermill":{"duration":0.030802,"end_time":"2020-09-29T17:35:42.718211","exception":false,"start_time":"2020-09-29T17:35:42.687409","status":"completed"},"tags":[],"id":"ejKOXtWkDV18","trusted":true},"cell_type":"code","source":"def augmentation_pipeline(image, label):\n    image = tf.image.random_flip_left_right(image)\n    return image, label","execution_count":9,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.76708Z","iopub.status.busy":"2020-09-29T17:35:42.766303Z","iopub.status.idle":"2020-09-29T17:35:42.769885Z","shell.execute_reply":"2020-09-29T17:35:42.769289Z"},"papermill":{"duration":0.030987,"end_time":"2020-09-29T17:35:42.770012","exception":false,"start_time":"2020-09-29T17:35:42.739025","status":"completed"},"tags":[],"id":"YCFaPMT3DV2M","trusted":true},"cell_type":"code","source":"def get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(augmentation_pipeline, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","execution_count":10,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.819725Z","iopub.status.busy":"2020-09-29T17:35:42.818931Z","iopub.status.idle":"2020-09-29T17:35:42.822335Z","shell.execute_reply":"2020-09-29T17:35:42.821619Z"},"papermill":{"duration":0.031482,"end_time":"2020-09-29T17:35:42.822461","exception":false,"start_time":"2020-09-29T17:35:42.790979","status":"completed"},"tags":[],"id":"VTp0EvnYDV2Z","trusted":true},"cell_type":"code","source":"def get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","execution_count":11,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.872445Z","iopub.status.busy":"2020-09-29T17:35:42.87135Z","iopub.status.idle":"2020-09-29T17:35:42.874919Z","shell.execute_reply":"2020-09-29T17:35:42.874172Z"},"papermill":{"duration":0.031324,"end_time":"2020-09-29T17:35:42.875045","exception":false,"start_time":"2020-09-29T17:35:42.843721","status":"completed"},"tags":[],"id":"Usfu_oSmDV2q","trusted":true},"cell_type":"code","source":"def get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","execution_count":12,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.92563Z","iopub.status.busy":"2020-09-29T17:35:42.924472Z","iopub.status.idle":"2020-09-29T17:35:42.927907Z","shell.execute_reply":"2020-09-29T17:35:42.927185Z"},"papermill":{"duration":0.031646,"end_time":"2020-09-29T17:35:42.92803","exception":false,"start_time":"2020-09-29T17:35:42.896384","status":"completed"},"tags":[],"id":"gGkwggYrDV23","trusted":true},"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","execution_count":13,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:42.979436Z","iopub.status.busy":"2020-09-29T17:35:42.978556Z","iopub.status.idle":"2020-09-29T17:35:42.982722Z","shell.execute_reply":"2020-09-29T17:35:42.982007Z"},"papermill":{"duration":0.032996,"end_time":"2020-09-29T17:35:42.982854","exception":false,"start_time":"2020-09-29T17:35:42.949858","status":"completed"},"tags":[],"id":"ERiUO9DIDV3O","outputId":"77972018-7df8-4cf4-d264-7224b74bceb6","trusted":true},"cell_type":"code","source":"NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nprint(\n    'Dataset: {} training images, {} validation images, {} unlabeled test images'.format(\n        NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES\n    )\n)","execution_count":14,"outputs":[{"output_type":"stream","text":"Dataset: 28984 training images, 4142 validation images, 10982 unlabeled test images\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"ðŸ“Œ **Cyclic learning rates :**\n\n<div class=\"alert alert-block alert-info\">\nCyclic learning rates  is a learning rate scheduling technique for (1) faster training of a network and (2) a finer understanding of the optimal learning rate. Cyclic learning rates have an effect on the model training process known somewhat fancifully as<code>\"superconvergence\"</code> \n</div>\n\nSource : @residentmario Notebook"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1bnsglJByd1lc1h9hGrz2WZa1LzVBgtWW)\n\nImage Source : https://arxiv.org/abs/1803.09820"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:43.036264Z","iopub.status.busy":"2020-09-29T17:35:43.035479Z","iopub.status.idle":"2020-09-29T17:35:43.039165Z","shell.execute_reply":"2020-09-29T17:35:43.038431Z"},"papermill":{"duration":0.034425,"end_time":"2020-09-29T17:35:43.039298","exception":false,"start_time":"2020-09-29T17:35:43.004873","status":"completed"},"tags":[],"id":"R7qwaU6HDV3f","trusted":true},"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.000075, lr_min=0.000001, lr_rampup_epochs=20, lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay ** (epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EfficientNets\n\n**Compound Model Scaling: A Better Way to Scale Up CNNs**\n\nEfficientNets uses a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient The first step in the compound scaling method is to perform a grid search to find the relationship between different scaling dimensions of the baseline network under a fixed resource constraint.This determines the appropriate scaling coefficient for each of the dimensions mentioned above. We then apply those coefficients to scale up the baseline network to the desired target model size or computational budget.\n\nThis compound scaling method consistently improves model accuracy and efficiency for scaling up existing models such as MobileNet (+1.4% imagenet accuracy), and ResNet (+0.7%), compared to conventional scaling methods.\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=18ZzZw0U-4O4iPe1JBVZwUVcn4FCmTS_x)"},{"metadata":{},"cell_type":"markdown","source":"**EfficientNet Architecture :**\n\nThe effectiveness of model scaling also relies heavily on the baseline network which  is developed by performing a neural architecture search using the AutoML MNAS framework, which optimizes both accuracy and efficiency (FLOPS). The resulting architecture uses mobile inverted bottleneck convolution (MBConv), similar to MobileNetV2 and MnasNet, but is slightly larger due to an increased FLOP budget.The baseline network is then scaled up to obtain a family of models, called EfficientNets."},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1KVGvxtN4XXvLbt3hCSSZBU04lv0LqgO-)"},{"metadata":{},"cell_type":"markdown","source":"ðŸ“Œ **EfficientNet Performance**\n\n<div class=\"alert alert-block alert-info\">\n<b>Note:</b>\nThe EfficientNet models achieve both higher accuracy and better efficiency over existing CNNs like ResNet , DenseNet etc , reducing parameter size and FLOPS by an order of magnitude. <code></code> \n</div>\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1jYu8AMPI9LVF2TQKUgJyKZaZqpsm3U0k)\n\nImage Source : EfficientNet Research Paper"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:35:43.098712Z","iopub.status.busy":"2020-09-29T17:35:43.097158Z","iopub.status.idle":"2020-09-29T17:36:47.655291Z","shell.execute_reply":"2020-09-29T17:36:47.654435Z"},"papermill":{"duration":64.593939,"end_time":"2020-09-29T17:36:47.655474","exception":false,"start_time":"2020-09-29T17:35:43.061535","status":"completed"},"tags":[],"id":"W9KECWJUDV3x","outputId":"a037aa1b-61a8-40f6-f7a3-6fb13de9f443","trusted":true},"cell_type":"code","source":"with strategy.scope():\n    \n    train_dataset = get_training_dataset()\n    valid_dataset = get_validation_dataset()\n    \n    model = tf.keras.Sequential([\n        efn.EfficientNetB6(\n            input_shape=(*IMAGE_SIZE, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(512, activation= 'relu'), \n        tf.keras.layers.Dropout(0.25), \n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n\nmodel.summary()","execution_count":16,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n165527552/165527152 [==============================] - 6s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nefficientnet-b6 (Model)      (None, 32, 32, 2304)      40960136  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2304)              0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               1180160   \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 513       \n=================================================================\nTotal params: 42,140,809\nTrainable params: 41,916,377\nNon-trainable params: 224,432\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:36:47.745093Z","iopub.status.busy":"2020-09-29T17:36:47.744217Z","iopub.status.idle":"2020-09-29T17:51:27.077691Z","shell.execute_reply":"2020-09-29T17:51:27.078803Z"},"papermill":{"duration":879.381253,"end_time":"2020-09-29T17:51:27.079066","exception":false,"start_time":"2020-09-29T17:36:47.697813","status":"completed"},"tags":[],"id":"C2H6UD7zDV4S","outputId":"dd30c1ce-8cde-41d9-ec47-75f5c7297f5c","trusted":true},"cell_type":"code","source":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE\n\n\nhistory = model.fit(\n    train_dataset, epochs=1,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset,\n    validation_steps=VALID_STEPS,\n    callbacks=[\n        tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1),\n        tf.keras.callbacks.ModelCheckpoint(\n            os.path.join(\"./model.h5\"),\n            monitor='train_loss', verbose=0,\n            save_best_only=True, save_weights_only=False,\n            mode='auto', save_freq='epoch'\n        )\n    ]\n)","execution_count":17,"outputs":[{"output_type":"stream","text":"\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n452/452 [==============================] - 647s 1s/step - accuracy: 0.9468 - loss: 0.2061 - val_accuracy: 0.9780 - val_loss: 0.1189 - lr: 1.0000e-05\n","name":"stdout"}]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:51:27.534548Z","iopub.status.busy":"2020-09-29T17:51:27.533826Z","iopub.status.idle":"2020-09-29T17:51:27.574098Z","shell.execute_reply":"2020-09-29T17:51:27.573357Z"},"papermill":{"duration":0.267563,"end_time":"2020-09-29T17:51:27.574233","exception":false,"start_time":"2020-09-29T17:51:27.30667","status":"completed"},"tags":[],"id":"DX-AS9uwDV4u","outputId":"f5370526-8616-4e5a-d826-8273c40b98c6","trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\nsub.head()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"     image_name  target\n0  ISIC_0052060       0\n1  ISIC_0052349       0\n2  ISIC_0058510       0\n3  ISIC_0073313       0\n4  ISIC_0073502       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0052060</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0052349</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0058510</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0073313</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0073502</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:51:28.035819Z","iopub.status.busy":"2020-09-29T17:51:28.034973Z","iopub.status.idle":"2020-09-29T17:52:57.150208Z","shell.execute_reply":"2020-09-29T17:52:57.151453Z"},"papermill":{"duration":89.353797,"end_time":"2020-09-29T17:52:57.15174","exception":false,"start_time":"2020-09-29T17:51:27.797943","status":"completed"},"tags":[],"id":"cG64ekdXDV4_","outputId":"e72ccc89-1144-4550-d9bf-5233c49de609","trusted":true},"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True)\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds)","execution_count":19,"outputs":[{"output_type":"stream","text":"Computing predictions...\n","name":"stdout"}]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:52:57.61423Z","iopub.status.busy":"2020-09-29T17:52:57.613347Z","iopub.status.idle":"2020-09-29T17:53:08.732233Z","shell.execute_reply":"2020-09-29T17:53:08.731524Z"},"papermill":{"duration":11.350758,"end_time":"2020-09-29T17:53:08.732368","exception":false,"start_time":"2020-09-29T17:52:57.38161","status":"completed"},"tags":[],"id":"toh9qEIHDV5O","outputId":"4616b3b6-24ef-40ab-af60-e1c1d07af4fb","trusted":true},"cell_type":"code","source":"print('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')","execution_count":20,"outputs":[{"output_type":"stream","text":"Generating submission.csv file...\n","name":"stdout"}]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:53:09.188252Z","iopub.status.busy":"2020-09-29T17:53:09.187462Z","iopub.status.idle":"2020-09-29T17:53:09.210035Z","shell.execute_reply":"2020-09-29T17:53:09.209303Z"},"papermill":{"duration":0.256696,"end_time":"2020-09-29T17:53:09.210164","exception":false,"start_time":"2020-09-29T17:53:08.953468","status":"completed"},"tags":[],"id":"Qhn36kErDV5y","outputId":"8950241e-1e43-41de-da98-330bece08cbd","trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame({'image_name': test_ids, 'target': np.concatenate(probabilities)})\npred_df.head()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"     image_name    target\n0  ISIC_6381819  0.019053\n1  ISIC_5583376  0.015064\n2  ISIC_6408546  0.004893\n3  ISIC_6932354  0.155703\n4  ISIC_8191278  0.018367","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_6381819</td>\n      <td>0.019053</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_5583376</td>\n      <td>0.015064</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_6408546</td>\n      <td>0.004893</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_6932354</td>\n      <td>0.155703</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_8191278</td>\n      <td>0.018367</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T17:53:09.683058Z","iopub.status.busy":"2020-09-29T17:53:09.668659Z","iopub.status.idle":"2020-09-29T17:53:09.817387Z","shell.execute_reply":"2020-09-29T17:53:09.816549Z"},"papermill":{"duration":0.382332,"end_time":"2020-09-29T17:53:09.817533","exception":false,"start_time":"2020-09-29T17:53:09.435201","status":"completed"},"tags":[],"id":"zUgVYFUIDV6D","outputId":"1245c3fb-5482-48a4-a9ca-33f1ab6f3142","trusted":true},"cell_type":"code","source":"del sub['target']\nsub = sub.merge(pred_df, on='image_name')\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"     image_name    target\n0  ISIC_0052060  0.063696\n1  ISIC_0052349  0.031600\n2  ISIC_0058510  0.052342\n3  ISIC_0073313  0.010814\n4  ISIC_0073502  0.024817","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0052060</td>\n      <td>0.063696</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0052349</td>\n      <td>0.031600</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0058510</td>\n      <td>0.052342</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0073313</td>\n      <td>0.010814</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0073502</td>\n      <td>0.024817</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# References :\n\nhttps://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n\nhttps://www.tensorflow.org/tutorials/load_data/tfrecord\n\nhttps://cloud.google.com/tpu\n\nhttps://www.tensorflow.org/guide/tpu\n\nhttps://www.tensorflow.org/tutorials/images/data_augmentation\n\nhttps://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}